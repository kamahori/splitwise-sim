_target_: model.AttentionArchitecture
name: mixtral-8x7b-attention
num_layers: 32
hidden_size: 1
experts_per_token: 2
